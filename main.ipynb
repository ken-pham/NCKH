{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/modify_configs.py --path /home/thinh_do/Workplace/ResGCNv1/resources  --ntu60_path /home/thinh_do/Workplace/ResGCNv1/NTU_RGBD/nturgbd_skeletons_s001_to_s017  --ntu120_path /home/thinh_do/Workplace/ResGCNv1/NTU_RGBD/nturgbd_skeletons_s018_to_s032\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2022-04-13 23:24:58,644 ] \n",
      "[ 2022-04-13 23:24:58,644 ] Starting generating ...\n",
      "[ 2022-04-13 23:24:58,644 ] Dataset: ntu-xsub\n",
      "[ 2022-04-13 23:24:58,644 ] Phase: train\n",
      "[ 2022-04-13 23:24:59,706 ] Phase: eval\n",
      "[ 2022-04-13 23:25:00,878 ] Finish generating!\n",
      "[ 2022-04-13 23:25:04,364 ] \n",
      "[ 2022-04-13 23:25:04,364 ] Starting generating ...\n",
      "[ 2022-04-13 23:25:04,364 ] Dataset: ntu-xview\n",
      "[ 2022-04-13 23:25:04,364 ] Phase: train\n",
      "[ 2022-04-13 23:25:05,366 ] Phase: eval\n",
      "[ 2022-04-13 23:25:06,547 ] Finish generating!\n",
      "[ 2022-04-13 23:25:10,019 ] \n",
      "[ 2022-04-13 23:25:10,019 ] Starting generating ...\n",
      "[ 2022-04-13 23:25:10,020 ] Dataset: ntu-xsub120\n",
      "[ 2022-04-13 23:25:10,020 ] Phase: train\n",
      "[ 2022-04-13 23:25:11,776 ] Phase: eval\n",
      "[ 2022-04-13 23:25:13,769 ] Finish generating!\n",
      "[ 2022-04-13 23:25:16,486 ] \n",
      "[ 2022-04-13 23:25:16,486 ] Starting generating ...\n",
      "[ 2022-04-13 23:25:16,486 ] Dataset: ntu-xset120\n",
      "[ 2022-04-13 23:25:16,486 ] Phase: train\n",
      "[ 2022-04-13 23:25:18,253 ] Phase: eval\n",
      "[ 2022-04-13 23:25:19,959 ] Finish generating!\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/auto_gen_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2022-04-13 23:25:42,628 ] \n",
      "[ 2022-04-13 23:25:42,628 ] Starting generating ...\n",
      "[ 2022-04-13 23:25:42,628 ] Dataset: ntu-xset120\n",
      "[ 2022-04-13 23:25:42,628 ] Phase: train\n",
      "[ 2022-04-13 23:25:43,742 ] Phase: eval\n",
      "[ 2022-04-13 23:25:44,919 ] Finish generating!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python main.py --config 1008 -v -e -vc 30 -pp /home/thinh-do/Workspace/ResGCNv1/pretrained/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *\n",
    "import os,time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path='/home/thinh_do/Workplace/ResGCNv1/pretrained'\n",
    "model=load_checkpoint1(path,model_name='1008_resgcn-b19_ntu-xset120.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,cv2\n",
    "import numpy as np\n",
    "from time import strftime\n",
    "import logging\n",
    "\n",
    "import threading\n",
    "\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "from torch import imag\n",
    "# from clearml import Task,Logger\n",
    "# from tensorboardX import SummaryWriter\n",
    "from Utils import make_landmark_timestep,init_parameters,detect,read_data,init_model,draw_class_on_image\n",
    "\n",
    "import imutils\n",
    "from imutils.video import FPS\n",
    "\n",
    "# task = Task.init(project_name='Action Recognition', task_name='task_1')\n",
    "# S_writer=SummaryWriter('run/Action Recognition')\n",
    "\n",
    "def main():\n",
    "    print ('--------------------------------')\n",
    "    parser = init_parameters()\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    net=init_model(args)\n",
    "    print(net)\n",
    "    label=\"NULL...\"\n",
    "    mpPose = mp.solutions.pose\n",
    "    pose = mpPose.Pose()\n",
    "    mpDraw = mp.solutions.drawing_utils\n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    path_video='/home/thinh_do/Workplace/ResGCNv1/1a.mp4'\n",
    "    print(\"================= Start detect ===============\")\n",
    "    video=cv2.VideoCapture(path_video)\n",
    "    time.sleep(1.0)\n",
    "    fps=FPS().start()\n",
    "    \n",
    "    while video.isOpened():\n",
    "        success,image=video.read()\n",
    "        imageRGB=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(imageRGB)\n",
    "        \n",
    "        \n",
    "        mpDraw.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mpPose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "        \n",
    "\n",
    "        image = draw_class_on_image(label, image)\n",
    "\n",
    "\n",
    "        fps.update()\n",
    "        fps.stop()\n",
    "        cv2.putText(image, \"FPS: {}\".format(int(fps.fps())), (10,70), cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0), 2)\n",
    "        cv2.imshow(\"Changed\", image)\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):  # press q to quit\n",
    "            break\n",
    "        \n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "video = cv2.VideoCapture('/home/thinh_do/Workplace/ResGCNv1/video.mp4')\n",
    "while video.isOpened():\n",
    "    ss,image=video.read()\n",
    "    print(image)\n",
    "    cv2.imshow(\"Changed\", image)\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):  # press q to quit\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    frame_paths, original_frames = frame_extraction(args.video)\n",
    "    num_frame = len(frame_paths)\n",
    "    h, w, _ = original_frames[0].shape\n",
    "\n",
    "    # Get Human detection results and pose results\n",
    "    human_detections = detection_inference(args, frame_paths)\n",
    "    pose_results = None\n",
    "    if args.use_skeleton_recog or args.use_skeleton_stdet:\n",
    "        pose_results = pose_inference(args, frame_paths, human_detections)\n",
    "\n",
    "    # resize frames to shortside 256\n",
    "    new_w, new_h = mmcv.rescale_size((w, h), (256, np.Inf))\n",
    "    frames = [mmcv.imresize(img, (new_w, new_h)) for img in original_frames]\n",
    "    w_ratio, h_ratio = new_w / w, new_h / h\n",
    "\n",
    "    # Load spatio-temporal detection label_map\n",
    "    stdet_label_map = load_label_map(args.label_map_stdet)\n",
    "    rgb_stdet_config = mmcv.Config.fromfile(args.rgb_stdet_config)\n",
    "    rgb_stdet_config.merge_from_dict(args.cfg_options)\n",
    "    try:\n",
    "        if rgb_stdet_config['data']['train']['custom_classes'] is not None:\n",
    "            stdet_label_map = {\n",
    "                id + 1: stdet_label_map[cls]\n",
    "                for id, cls in enumerate(rgb_stdet_config['data']['train']\n",
    "                                         ['custom_classes'])\n",
    "            }\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    action_result = None\n",
    "    if args.use_skeleton_recog:\n",
    "        print('Use skeleton-based recognition')\n",
    "        action_result = skeleton_based_action_recognition(\n",
    "            args, pose_results, num_frame, h, w)\n",
    "    else:\n",
    "        print('Use rgb-based recognition')\n",
    "        action_result = rgb_based_action_recognition(args)\n",
    "\n",
    "    stdet_preds = None\n",
    "    if args.use_skeleton_stdet:\n",
    "        print('Use skeleton-based SpatioTemporal Action Detection')\n",
    "        clip_len, frame_interval = 30, 1\n",
    "        timestamps, stdet_preds = skeleton_based_stdet(args, stdet_label_map,\n",
    "                                                       human_detections,\n",
    "                                                       pose_results, num_frame,\n",
    "                                                       clip_len,\n",
    "                                                       frame_interval, h, w)\n",
    "        for i in range(len(human_detections)):\n",
    "            det = human_detections[i]\n",
    "            det[:, 0:4:2] *= w_ratio\n",
    "            det[:, 1:4:2] *= h_ratio\n",
    "            human_detections[i] = torch.from_numpy(det[:, :4]).to(args.device)\n",
    "\n",
    "    else:\n",
    "        print('Use rgb-based SpatioTemporal Action Detection')\n",
    "        for i in range(len(human_detections)):\n",
    "            det = human_detections[i]\n",
    "            det[:, 0:4:2] *= w_ratio\n",
    "            det[:, 1:4:2] *= h_ratio\n",
    "            human_detections[i] = torch.from_numpy(det[:, :4]).to(args.device)\n",
    "        timestamps, stdet_preds = rgb_based_stdet(args, frames,\n",
    "                                                  stdet_label_map,\n",
    "                                                  human_detections, w, h,\n",
    "                                                  new_w, new_h, w_ratio,\n",
    "                                                  h_ratio)\n",
    "\n",
    "    stdet_results = []\n",
    "    for timestamp, prediction in zip(timestamps, stdet_preds):\n",
    "        human_detection = human_detections[timestamp - 1]\n",
    "        stdet_results.append(\n",
    "            pack_result(human_detection, prediction, new_h, new_w))\n",
    "\n",
    "    def dense_timestamps(timestamps, n):\n",
    "        \"\"\"Make it nx frames.\"\"\"\n",
    "        old_frame_interval = (timestamps[1] - timestamps[0])\n",
    "        start = timestamps[0] - old_frame_interval / n * (n - 1) / 2\n",
    "        new_frame_inds = np.arange(\n",
    "            len(timestamps) * n) * old_frame_interval / n + start\n",
    "        return new_frame_inds.astype(np.int)\n",
    "\n",
    "    dense_n = int(args.predict_stepsize / args.output_stepsize)\n",
    "    output_timestamps = dense_timestamps(timestamps, dense_n)\n",
    "    frames = [\n",
    "        cv2.imread(frame_paths[timestamp - 1])\n",
    "        for timestamp in output_timestamps\n",
    "    ]\n",
    "\n",
    "    print('Performing visualization')\n",
    "    pose_model = init_pose_model(args.pose_config, args.pose_checkpoint,\n",
    "                                 args.device)\n",
    "\n",
    "    if args.use_skeleton_recog or args.use_skeleton_stdet:\n",
    "        pose_results = [\n",
    "            pose_results[timestamp - 1] for timestamp in output_timestamps\n",
    "        ]\n",
    "\n",
    "    vis_frames = visualize(frames, stdet_results, pose_results, action_result,\n",
    "                           pose_model)\n",
    "    vid = mpy.ImageSequenceClip([x[:, :, ::-1] for x in vis_frames],\n",
    "                                fps=args.output_fps)\n",
    "    vid.write_videofile(args.out_filename)\n",
    "\n",
    "    tmp_frame_dir = osp.dirname(frame_paths[0])\n",
    "    shutil.rmtree(tmp_frame_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.path.dirname(os.path.abspath(__file__))+\"/\"\n",
    "    sys.path.append(ROOT)\n",
    "    cfg_all = read_yaml(ROOT +\"class.yaml\")\n",
    "    CLASSES = np.array(cfg_all[\"classes\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
